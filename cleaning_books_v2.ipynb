{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import json\n",
    "import re\n",
    "from dateutil.parser import parse\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating auxiliary function\n",
    "def export_json(data, filename):    \n",
    "    with open(f'./output/{filename}.json', 'w') as f:\n",
    "        json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "Columns of dataset\n",
      "Index(['bookId', 'title', 'series', 'author', 'rating', 'isbn', 'genres',\n",
      "       'pages', 'publishDate', 'firstPublishDate', 'awards', 'numRatings',\n",
      "       'ratingsByStars'],\n",
      "      dtype='object')\n",
      "-> Lenght before removing books without ISBN: 52478\n",
      "-> Lenght after removing books without ISBN: 48124\n",
      "-> Lenght before filtering 0.5 best books: 48124\n",
      "-> Lenght after filtering 0.5 best books: 24060\n",
      "-> Lenght before filtering by weird number of pages: 24060\n",
      "-> Lenght after filtering by weird number of pages: 23711\n"
     ]
    }
   ],
   "source": [
    "# Importing the data\n",
    "df_books = pd.read_csv('input/best_books_goodreads.csv', dtype={'awards':object})\n",
    "print(len(df_books.columns))\n",
    "\n",
    "# Dropping unnecessary columns\n",
    "df_books.drop(columns=['description', 'language', 'characters', 'bookFormat','edition','publisher','likedPercent',\n",
    "                        'setting', 'coverImg', 'bbeScore', 'bbeVotes', 'price'], inplace=True)\n",
    "\n",
    "print('Columns of dataset')\n",
    "print(df_books.columns)\n",
    "\n",
    "# Removing books without ISBN\n",
    "print('-> Lenght before removing books without ISBN:',len(df_books))\n",
    "df_books = df_books[df_books['isbn']!= '9999999999999']\n",
    "print('-> Lenght after removing books without ISBN:',len(df_books))\n",
    "\n",
    "# Removing books with low quantity of ratings\n",
    "median_num_ratings = df_books['numRatings'].fillna(value=0).astype(int).quantile(0.5)\n",
    "print(\"-> Lenght before filtering 0.5 best books:\", len(df_books))\n",
    "df_books = df_books[df_books['numRatings'] > median_num_ratings].copy()\n",
    "print(\"-> Lenght after filtering 0.5 best books:\", len(df_books))\n",
    "\n",
    "# Removing books with weird or uncommon page counts\n",
    "print(\"-> Lenght before filtering by weird number of pages:\", len(df_books))\n",
    "df_books = df_books[df_books['pages']!='1 page'] # '1 page' error\n",
    "df_books = df_books[~df_books['pages'].isna()] # NA error\n",
    "df_books = df_books[df_books['pages'] != 0] # Books with 0 pages\n",
    "min_num_of_pages = 10\n",
    "max_num_of_pages = 2000 \n",
    "df_books['pages'] = df_books['pages'].astype(int)\n",
    "df_books = df_books[df_books['pages'] < max_num_of_pages] # Removing books with less than 10 pages\n",
    "df_books = df_books[df_books['pages'] > min_num_of_pages] # Removing books with more than 2000 pages\n",
    "print(\"-> Lenght after filtering by weird number of pages:\",len(df_books))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23695"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_books)\n",
    "df_books.drop_duplicates(inplace=True)\n",
    "len(df_books)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Lenght before filtering null or weird dates: 23695\n",
      "-> Lenght after filtering null or weird dates: 22957\n"
     ]
    }
   ],
   "source": [
    "# Function to clean date\n",
    "def clean_date(date_str):\n",
    "    try:\n",
    "        # If the date contains only a year, returns None\n",
    "        if re.match(r'^\\d{4}$', date_str):\n",
    "            return None\n",
    "        \n",
    "        # If the date is in MM/DD/YY format\n",
    "        if re.match(r'\\d{2}/\\d{2}/\\d{2}', date_str):\n",
    "            parsed_date = pd.to_datetime(date_str, format='%m/%d/%y')\n",
    "        else:\n",
    "            # If the date is in a text format, use the date parser\n",
    "            parsed_date = parse(date_str)\n",
    "        \n",
    "        # If the result date is after than 2020, correct it to 19XX\n",
    "        if parsed_date >= datetime.strptime('2021-01-01', '%Y-%m-%d'):\n",
    "            parsed_date = parsed_date.replace(year=parsed_date.year - 100)\n",
    "        \n",
    "        # Returns date in YYYY-MM-DD format\n",
    "        return parsed_date.strftime('%Y-%m-%d')\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "df_books['best_publish_date'] = df_books['firstPublishDate'].fillna(df_books['publishDate'])\n",
    "\n",
    "# Apply the function to the 'dirty_date' column\n",
    "df_books['clean_date'] = df_books['best_publish_date'].apply(clean_date)\n",
    "\n",
    "# Removes books with no publishing date\n",
    "print(\"-> Lenght before filtering null or weird dates:\",len(df_books))\n",
    "df_books = df_books[~df_books['clean_date'].isna()]\n",
    "\n",
    "# Removes books with weird date\n",
    "df_books = df_books[df_books['clean_date']!='0458-09-26']\n",
    "print(\"-> Lenght after filtering null or weird dates:\",len(df_books))\n",
    "\n",
    "# Calculates day of the week and month based on publishing date\n",
    "df_books['clean_date'] = pd.to_datetime(df_books['clean_date'])\n",
    "df_books['day_of_week'] = df_books['clean_date'].dt.day_name().astype(str)\n",
    "df_books['month_name'] = df_books['clean_date'].dt.month_name().astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting the genres column\n",
    "df_books['genres'] = df_books['genres'].apply(literal_eval)\n",
    "\n",
    "# Filtering the genre with a hand tailored list\n",
    "valid_genres = ['Fiction', 'Nonfiction', 'Mystery', 'Fantasy', 'Science Fiction', 'Romance', 'Biography', 'Historical Fiction', 'Young Adult', 'Childrens', 'Self Help', 'Horror', 'Classics', 'Poetry', 'Graphic Novels', 'Adventure', 'True Crime', 'Religion', 'Science', 'Business']\n",
    "df_books['valid_genres'] = df_books['genres'].apply(lambda x: [item for item in x if item in valid_genres])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating success metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating normalized rating column\n",
    "df_books['norm_rating'] = df_books['rating']/5\n",
    "\n",
    "# Calculating number of awards per book\n",
    "df_books['awards'] = df_books['awards'].apply(literal_eval)\n",
    "df_books['num_awards'] = df_books['awards'].apply(len)\n",
    "\n",
    "# Creating normalized number of awards column\n",
    "max_num_awards = max(df_books['num_awards'])\n",
    "df_books['norm_num_awards'] = df_books['num_awards']/max_num_awards\n",
    "\n",
    "# Creating normalized number of ratings column\n",
    "max_num_ratings = max(df_books['numRatings'])\n",
    "df_books['norm_num_ratings'] = df_books['numRatings']/max_num_ratings\n",
    "\n",
    "# Defining the weights for a general success metric\n",
    "weights = {\n",
    "    'avg_rating': 1,\n",
    "    'num_awards': 2,\n",
    "    'num_ratings': 1\n",
    "}\n",
    "\n",
    "# Calculating the success rate\n",
    "df_books['success_rate'] = (df_books['norm_num_awards']*weights['num_awards']+\\\n",
    "                           df_books['norm_num_ratings']*weights['num_ratings']+\\\n",
    "                           df_books['norm_rating']*weights['avg_rating'])/sum(weights.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating auxiliary objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Success metrics by number of pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculating the average success rate by number of pages\n",
    "# success_by_num_pages = df_books.groupby('pages').agg({\n",
    "#     'success_rate':'mean',\n",
    "#     'norm_rating':'mean',\n",
    "#     'norm_num_awards':'mean',\n",
    "#     'norm_num_ratings':'mean',\n",
    "#     })\n",
    "\n",
    "# success_by_num_pages.to_csv('output/num_pages_analysis/sucess_by_num_of_pages.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of books by time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculating and exporting number of books by date\n",
    "# num_book_by_date = df_books.groupby('clean_date').agg({'bookId': 'count'})\n",
    "# num_book_by_date.rename(columns={'bookId': 'num_of_books'}, inplace=True)\n",
    "# num_book_by_date.to_csv('./output/num_books_by_time/num_book_by_date.csv')\n",
    "\n",
    "# # Calculating and exporting number of books by day of week\n",
    "# num_book_by_day_of_week = df_books.groupby('day_of_week').agg({'bookId': 'count'})\n",
    "# num_book_by_day_of_week.rename(columns={'bookId': 'num_of_books'}, inplace=True)\n",
    "# num_book_by_day_of_week.to_csv('./output/num_books_by_time/num_book_by_day_of_week.csv')\n",
    "\n",
    "# # Calculating and exporting number of books by month name\n",
    "# num_book_by_month = df_books.groupby('month_name').agg({'bookId': 'count'})\n",
    "# num_book_by_month.rename(columns={'bookId': 'num_of_books'}, inplace=True)\n",
    "# new_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "# num_book_by_month = num_book_by_month.reindex(new_order, axis=0)\n",
    "# num_book_by_month.to_csv('./output/num_books_by_time/num_book_by_month.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Books and book count by genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the books for each genre\n",
    "genre_dict = defaultdict(list)\n",
    "for index, row in df_books.iterrows():\n",
    "    book_name = row['bookId']\n",
    "    genres = row['valid_genres']\n",
    "    for genre in genres:\n",
    "        genre_dict[genre].append(book_name)\n",
    "\n",
    "books_by_genre_dict = dict(genre_dict)\n",
    "\n",
    "# Get the number of books for each genre and sort it\n",
    "genre_count = {k: len(v) for k, v in books_by_genre_dict.items()}\n",
    "genre_count = {k: v for k, v in sorted(genre_count.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "# Saving number of books for each genre and books by genre\n",
    "export_json(genre_count, 'genres/num_books_by_genre')\n",
    "export_json(books_by_genre_dict, 'genres/books_by_genre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating object with genres co-occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize nested defaultdict for counting co-occurrences\n",
    "co_occurrence = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Count co-occurrences for each genre combination\n",
    "for genre1 in books_by_genre_dict:\n",
    "    for genre2 in books_by_genre_dict:\n",
    "        if genre1 == genre2:\n",
    "            continue\n",
    "        common_books = set(books_by_genre_dict[genre1]) & set(books_by_genre_dict[genre2])\n",
    "        co_occurrence[genre1][genre2] = len(common_books)\n",
    "\n",
    "# Convert nested defaultdict to regular dict\n",
    "co_occurrence = {k: dict(v) for k, v in co_occurrence.items()}\n",
    "\n",
    "# Saving co-occurence counts\n",
    "export_json(co_occurrence, 'genres/cooccurences_2_by_2_genre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting trends in series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataframe with books that are series only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating columns with the name and position in series\n",
    "df_series = (df_books[~df_books['series'].isna()]).copy()\n",
    "df_series['name_of_series'] = df_series['series'].str.split(pat='#', expand=True)[0]\n",
    "df_series['num_in_series'] = df_series['series'].str.split(pat='#', expand=True)[1]\n",
    "# df_series.head(3)\n",
    "\n",
    "# Filtering for coherent series numbers (from 1 to 20)\n",
    "valid_series = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20']\n",
    "df_series = df_series[df_series['num_in_series'].isin(valid_series)].copy()\n",
    "df_series['num_in_series'] = df_series['num_in_series'].astype(int)\n",
    "series_information = df_series[['bookId', 'name_of_series', 'num_in_series']]\n",
    "df_books = df_books.merge(series_information, how='left', on='bookId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating auxiliary objects for the series dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculating and exporting success rates by number in series\n",
    "# sucess_by_num_in_series = df_series.groupby('num_in_series', as_index=True).agg({\n",
    "#     'success_rate':'mean',\n",
    "#     'norm_rating':'mean',\n",
    "#     'norm_num_awards':'mean',\n",
    "#     'norm_num_ratings':'mean',\n",
    "# })\n",
    "# sucess_by_num_in_series.to_csv('./output/series_analysis/sucess_by_num_in_series.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding information from webscrapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding clean author information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22484\n",
      "22455\n"
     ]
    }
   ],
   "source": [
    "df_openlib = pd.read_csv('input/df_books_open_library.csv')\n",
    "print(len(df_openlib))\n",
    "df_openlib.drop_duplicates(inplace=True)\n",
    "print(len(df_openlib))\n",
    "new_author_map = df_openlib[['bookId', 'new_author']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22957\n",
      "22957\n"
     ]
    }
   ],
   "source": [
    "print(len(df_books))\n",
    "df_books = df_books.merge(new_author_map, on='bookId', how='left')\n",
    "print(len(df_books))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3292"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_books['new_author'] = df_books['new_author'].apply(lambda item: item if item != \"Author not found\" else None)\n",
    "len(df_books[df_books['new_author'].isna()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the original author column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning author column\n",
    "df_books['clean_author'] = df_books['author'].str.split(pat=',', expand=True)[0]\n",
    "\n",
    "#  Function to remove text inside parentheses\n",
    "def remove_parentheses(text):\n",
    "    return re.sub(r'\\([^)]*\\)', '', text)\n",
    "\n",
    "# Apply the function to the column\n",
    "df_books['clean_author'] = df_books['clean_author'].apply(remove_parentheses)\n",
    "\n",
    "# Remove extra spaces if needed\n",
    "df_books['clean_author'] = df_books['clean_author'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding geolocation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22957\n"
     ]
    }
   ],
   "source": [
    "# Reading geolocation data\n",
    "df_geolocation = pd.read_csv('input/birthplace_result.csv')\n",
    "df_geolocation.drop_duplicates(inplace=True)\n",
    "print(len(df_geolocation))\n",
    "map_geolocation = df_geolocation[['bookId', 'birthplace', 'latlong', 'author_used']]\n",
    "df_books = df_books.merge(map_geolocation, on='bookId', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bookId', 'title', 'series', 'author', 'rating', 'isbn', 'genres',\n",
       "       'pages', 'publishDate', 'firstPublishDate', 'awards', 'numRatings',\n",
       "       'ratingsByStars', 'best_publish_date', 'clean_date', 'day_of_week',\n",
       "       'month_name', 'valid_genres', 'norm_rating', 'num_awards',\n",
       "       'norm_num_awards', 'norm_num_ratings', 'success_rate', 'name_of_series',\n",
       "       'num_in_series', 'new_author', 'clean_author', 'birthplace', 'latlong',\n",
       "       'author_used'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_books.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books[['latitude', 'longitude']] = df_books['latlong'].str.split(' ', expand=True)\n",
    "df_books['author'] = df_books.apply(lambda row: row['new_author'] if row['author_used'] == 2 else row['clean_author'], axis=1)\n",
    "df_books['genres'] = df_books['valid_genres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books.drop(columns=['publishDate', 'firstPublishDate', 'ratingsByStars', 'best_publish_date','valid_genres', 'clean_author', 'new_author', 'author_used', 'latlong'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bookId', 'title', 'series', 'author', 'rating', 'isbn', 'genres',\n",
       "       'pages', 'awards', 'numRatings', 'clean_date', 'day_of_week',\n",
       "       'month_name', 'norm_rating', 'num_awards', 'norm_num_awards',\n",
       "       'norm_num_ratings', 'success_rate', 'name_of_series', 'num_in_series',\n",
       "       'birthplace', 'latitude', 'longitude'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_books.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books.to_csv('output/output_dfs/clean_books.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tecnico_sandbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
